{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "burVInMc4hzI",
        "outputId": "bb008c47-2500-463f-94ba-36847c67445d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_PYzx5zAmkY"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0f8jMLekh89e",
        "outputId": "8d216c06-9540-4289-d500-cd0d4e96af65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.12/dist-packages (0.1.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.9)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.0.15)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (0.23.0+cu126)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.2.1->lpips) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.3)\n",
            "Requirement already satisfied: pytorch-fid in /usr/local/lib/python3.12/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pytorch-ssim in /usr/local/lib/python3.12/dist-packages (0.1)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.12/dist-packages (0.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (3.0.3)\n",
            "Requirement already satisfied: piq in /usr/local/lib/python3.12/dist-packages (0.8.0)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from piq) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.10.0->piq) (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.10.0->piq) (2.8.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.10.0->piq) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision>=0.10.0->piq) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision>=0.10.0->piq) (3.0.3)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.12/dist-packages (0.1.4)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.16.2)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.2.1->lpips) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision tqdm requests\n",
        "!pip install einops tqdm\n",
        "!pip install albumentations lpips tqdm\n",
        "!pip install pytorch-fid pytorch-ssim lpips\n",
        "!pip install piq\n",
        "!pip install lpips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAxrYaR__EcS",
        "outputId": "d9d200b9-30b9-4a0d-a753-7537628665f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from torchvision.models import vgg16\n",
        "from torchvision import utils as vutils\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import albumentations\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import hashlib\n",
        "import pickle\n",
        "import uuid\n",
        "import argparse\n",
        "\n",
        "from tqdm import tqdm\n",
        "import scipy.linalg\n",
        "from piq import ssim, psnr\n",
        "# from LPIPS import LPIPS\n",
        "from scipy.spatial import distance\n",
        "from collections import namedtuple\n",
        "import requests\n",
        "import shutil\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim_metric, peak_signal_noise_ratio as psnr_metric\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "import lpips\n",
        "lpips_fn = lpips.LPIPS(net='alex').eval().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VrDIgY3A3lB"
      },
      "source": [
        "## Configuraciones iniciales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H7mplOZtArAO"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Configuraci√≥n de rutas\n",
        "# ============================\n",
        "BASE_CKPT_DIR = \"/content/drive/MyDrive/Proyecto_Grado/VQ_GAN/VQCheckpointsVQ1\"\n",
        "GEN_IMG_DIR = \"/content/drive/MyDrive/Proyecto_Grado/VQ_GAN/generatedVQ1\"\n",
        "METRICS_PATH = \"/content/drive/MyDrive/Proyecto_Grado/VQ_GAN/metrics_history.json\"\n",
        "LOSS_VQ_PATH = \"/content/drive/MyDrive/Proyecto_Grado/VQ_GAN/loss_history_vq.json\"\n",
        "LOSS_TRANS_PATH = \"/content/drive/MyDrive/Proyecto_Grado/VQ_GAN/loss_history_transformer.json\"\n",
        "\n",
        "os.makedirs(BASE_CKPT_DIR, exist_ok=True)\n",
        "os.makedirs(GEN_IMG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxBA0wscpM8_"
      },
      "source": [
        "# Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xbwO5137o5Ru"
      },
      "outputs": [],
      "source": [
        "# helper.py\n",
        "class GroupNorm(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(GroupNorm, self).__init__()\n",
        "        self.gn = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gn(x)\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.block = nn.Sequential(\n",
        "            GroupNorm(in_channels),\n",
        "            Swish(),\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            GroupNorm(out_channels),\n",
        "            Swish(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.channel_up = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.in_channels != self.out_channels:\n",
        "            return self.channel_up(x) + self.block(x)\n",
        "        else:\n",
        "            return x + self.block(x)\n",
        "\n",
        "\n",
        "class UpSampleBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(UpSampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DownSampleBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(DownSampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, 2, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (0, 1, 0, 1)\n",
        "        x = F.pad(x, pad, mode=\"constant\", value=0)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class NonLocalBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(NonLocalBlock, self).__init__()\n",
        "        self.in_channels = channels\n",
        "\n",
        "        self.gn = GroupNorm(channels)\n",
        "        self.q = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.k = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.v = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.proj_out = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = self.gn(x)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        b, c, h, w = q.shape\n",
        "\n",
        "        q = q.reshape(b, c, h*w)\n",
        "        q = q.permute(0, 2, 1)\n",
        "        k = k.reshape(b, c, h*w)\n",
        "        v = v.reshape(b, c, h*w)\n",
        "\n",
        "        attn = torch.bmm(q, k)\n",
        "        attn = attn * (int(c)**(-0.5))\n",
        "        attn = F.softmax(attn, dim=2)\n",
        "        attn = attn.permute(0, 2, 1)\n",
        "\n",
        "        A = torch.bmm(v, attn)\n",
        "        A = A.reshape(b, c, h, w)\n",
        "\n",
        "        return x + A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Re1SNNdVpIMT"
      },
      "outputs": [],
      "source": [
        "# codebook.py\n",
        "class Codebook(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Codebook, self).__init__()\n",
        "        self.num_codebook_vectors = args.num_codebook_vectors\n",
        "        self.latent_dim = args.latent_dim\n",
        "        self.beta = args.beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.num_codebook_vectors, self.latent_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.num_codebook_vectors, 1.0 / self.num_codebook_vectors)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z.view(-1, self.latent_dim)\n",
        "\n",
        "        d = torch.sum(z_flattened**2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - \\\n",
        "            2*(torch.matmul(z_flattened, self.embedding.weight.t()))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        loss = torch.mean((z_q.detach() - z)**2) + self.beta * torch.mean((z_q - z.detach())**2)\n",
        "\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        z_q = z_q.permute(0, 3, 1, 2)\n",
        "\n",
        "        return z_q, min_encoding_indices, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u4gLgfHqpbYM"
      },
      "outputs": [],
      "source": [
        "# Encoder / Decoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "        # channels = [128, 128, 128, 256, 256, 512]\n",
        "        channels = [64, 128, 256]\n",
        "        attn_resolutions = [16]\n",
        "        num_res_blocks = 2\n",
        "        resolution = 256\n",
        "        layers = [nn.Conv2d(args.image_channels, channels[0], 3, 1, 1)]\n",
        "        for i in range(len(channels)-1):\n",
        "            in_channels = channels[i]\n",
        "            out_channels = channels[i+1]\n",
        "            for j in range(num_res_blocks):\n",
        "                layers.append(ResidualBlock(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "            if resolution in attn_resolutions:\n",
        "                # layers.append(NonLocalBlock(in_channels))\n",
        "                pass\n",
        "            if i != len(channels)-2:\n",
        "                layers.append(DownSampleBlock(channels[i+1]))\n",
        "            resolution //= 2\n",
        "        layers += [\n",
        "            ResidualBlock(channels[-1], channels[-1]),\n",
        "            # NonLocalBlock(channels[-1]),\n",
        "            ResidualBlock(channels[-1], channels[-1]),\n",
        "            GroupNorm(channels[-1]),\n",
        "            Swish(),\n",
        "            nn.Conv2d(channels[-1], args.latent_dim, 3, 1, 1)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "        channels = [256, 128, 64]\n",
        "        attn_resolutions = [16]\n",
        "        num_res_blocks = 2\n",
        "        resolution = args.image_size // 4  # empezar en 32 si la imagen es 128\n",
        "\n",
        "        in_channels = channels[0]\n",
        "        layers = [\n",
        "            nn.Conv2d(args.latent_dim, in_channels, 3, 1, 1),\n",
        "            ResidualBlock(in_channels, in_channels),\n",
        "            ResidualBlock(in_channels, in_channels)\n",
        "        ]\n",
        "\n",
        "        for i in range(len(channels)):\n",
        "            out_channels = channels[i]\n",
        "            for j in range(num_res_blocks):\n",
        "                layers.append(ResidualBlock(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "\n",
        "            if resolution in attn_resolutions:\n",
        "                layers.append(NonLocalBlock(in_channels))\n",
        "\n",
        "            if i != 0 and resolution < args.image_size:\n",
        "                layers.append(UpSampleBlock(in_channels))\n",
        "                resolution *= 2\n",
        "\n",
        "        layers += [\n",
        "            GroupNorm(in_channels),\n",
        "            Swish(),\n",
        "            nn.Conv2d(in_channels, args.image_channels, 3, 1, 1)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WqDCC2cYpejx"
      },
      "outputs": [],
      "source": [
        "# discriminator.py\n",
        "\"\"\"\n",
        "PatchGAN Discriminator (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L538)\n",
        "\"\"\"\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, args, num_filters_last=64, n_layers=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(args.image_channels, num_filters_last, 4, 2, 1), nn.LeakyReLU(0.2)]\n",
        "        num_filters_mult = 1\n",
        "\n",
        "        for i in range(1, n_layers + 1):\n",
        "            num_filters_mult_last = num_filters_mult\n",
        "            num_filters_mult = min(2 ** i, 8)\n",
        "            layers += [\n",
        "                nn.Conv2d(num_filters_last * num_filters_mult_last, num_filters_last * num_filters_mult, 4,\n",
        "                          2 if i < n_layers else 1, 1, bias=False),\n",
        "                nn.BatchNorm2d(num_filters_last * num_filters_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        layers.append(nn.Conv2d(num_filters_last * num_filters_mult, 1, 4, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DpqY10taq6io"
      },
      "outputs": [],
      "source": [
        "# mingpt.py\n",
        "\"\"\"\n",
        "taken from: https://github.com/karpathy/minGPT/\n",
        "GPT model:\n",
        "- the initial stem consists of a combination of token encoding and a positional encoding\n",
        "- the meat of it is a uniform sequence of Transformer blocks\n",
        "    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
        "    - all blocks feed into a central residual pathway similar to resnets\n",
        "- the final decoder is a linear projection into a vanilla Softmax classifier\n",
        "\"\"\"\n",
        "class GPTConfig:\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        mask = torch.tril(torch.ones(config.block_size,\n",
        "                                     config.block_size))\n",
        "        if hasattr(config, \"n_unmasked\"):\n",
        "            mask[:config.n_unmasked, :config.n_unmasked] = 1\n",
        "        self.register_buffer(\"mask\", mask.view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        present = torch.stack((k, v))\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past\n",
        "            k = torch.cat((past_key, k), dim=-2)\n",
        "            v = torch.cat((past_value, v), dim=-2)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        if layer_past is None:\n",
        "            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y, present  # TODO: check that this does not break anything\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),  # nice\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, layer_past=None, return_present=False):\n",
        "        # TODO: check that training still works\n",
        "        if return_present:\n",
        "            assert not self.training\n",
        "        # layer past: tuple of length two with B, nh, T, hs\n",
        "        attn, present = self.attn(self.ln1(x), layer_past=layer_past)\n",
        "\n",
        "        x = x + attn\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        if layer_past is not None or return_present:\n",
        "            return x, present\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, n_layer=12, n_head=8, n_embd=256,\n",
        "                 embd_pdrop=0., resid_pdrop=0., attn_pdrop=0., n_unmasked=0):\n",
        "        super().__init__()\n",
        "        config = GPTConfig(vocab_size=vocab_size, block_size=block_size,\n",
        "                           embd_pdrop=embd_pdrop, resid_pdrop=resid_pdrop, attn_pdrop=attn_pdrop,\n",
        "                           n_layer=n_layer, n_head=n_head, n_embd=n_embd,\n",
        "                           n_unmasked=n_unmasked)\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))  # 512 x 1024\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "        self.config = config\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, idx, embeddings=None):\n",
        "        token_embeddings = self.tok_emb(idx)  # each index maps to a (learnable) vector\n",
        "\n",
        "        if embeddings is not None:  # prepend explicit embeddings\n",
        "            token_embeddings = torch.cat((embeddings, token_embeddings), dim=1)\n",
        "\n",
        "        t = token_embeddings.shape[1]\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "        position_embeddings = self.pos_emb[:, :t, :]  # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        return logits, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q3T-kSphq9WX"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "# utils.py\n",
        "\n",
        "# --------------------------------------------- #\n",
        "#                  Data Utils\n",
        "# --------------------------------------------- #\n",
        "\n",
        "class ImagePaths(Dataset):\n",
        "    def __init__(self, path, size=None):\n",
        "        self.size = size\n",
        "\n",
        "        self.images = [os.path.join(path, file) for file in os.listdir(path)]\n",
        "        self._length = len(self.images)\n",
        "\n",
        "        self.rescaler = albumentations.SmallestMaxSize(max_size=self.size)\n",
        "        self.cropper = albumentations.CenterCrop(height=self.size, width=self.size)\n",
        "        self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except (UnidentifiedImageError, OSError):\n",
        "            print(f\"‚ö†Ô∏è Imagen inv√°lida: {image_path}, reemplazada por ruido\")\n",
        "            image = Image.fromarray(\n",
        "                np.uint8(np.random.rand(self.size, self.size, 3) * 255)\n",
        "            )\n",
        "\n",
        "        image = np.array(image).astype(np.uint8)\n",
        "        image = self.preprocessor(image=image)[\"image\"]\n",
        "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        example = self.preprocess_image(self.images[i])\n",
        "        return example\n",
        "\n",
        "\n",
        "def load_data(args):\n",
        "    train_data = ImagePaths(args.dataset_path, size=128)\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True, persistent_workers=True)\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "# --------------------------------------------- #\n",
        "#                  Module Utils\n",
        "#            for Encoder, Decoder etc.\n",
        "# --------------------------------------------- #\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "def plot_images(images):\n",
        "    x = images[\"input\"]\n",
        "    reconstruction = images[\"rec\"]\n",
        "    half_sample = images[\"half_sample\"]\n",
        "    full_sample = images[\"full_sample\"]\n",
        "\n",
        "    fig, axarr = plt.subplots(1, 4)\n",
        "    axarr[0].imshow(x.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[1].imshow(reconstruction.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[2].imshow(half_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[3].imshow(full_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DQq6HIJ_rMC6"
      },
      "outputs": [],
      "source": [
        "# vqgan.py\n",
        "class VQGAN(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(VQGAN, self).__init__()\n",
        "        self.encoder = Encoder(args).to(device=args.device)\n",
        "        self.decoder = Decoder(args).to(device=args.device)\n",
        "        self.codebook = Codebook(args).to(device=args.device)\n",
        "        self.quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
        "        self.post_quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        encoded_images = self.encoder(imgs)\n",
        "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
        "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
        "        post_quant_conv_mapping = self.post_quant_conv(codebook_mapping)\n",
        "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
        "\n",
        "        return decoded_images, codebook_indices, q_loss\n",
        "\n",
        "    def encode(self, imgs):\n",
        "        encoded_images = self.encoder(imgs)\n",
        "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
        "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
        "        return codebook_mapping, codebook_indices, q_loss\n",
        "\n",
        "    def decode(self, z):\n",
        "        post_quant_conv_mapping = self.post_quant_conv(z)\n",
        "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
        "        return decoded_images\n",
        "\n",
        "    def calculate_lambda(self, perceptual_loss, gan_loss):\n",
        "        last_layer = self.decoder.model[-1]\n",
        "        last_layer_weight = last_layer.weight\n",
        "        perceptual_loss_grads = torch.autograd.grad(perceptual_loss, last_layer_weight, retain_graph=True)[0]\n",
        "        gan_loss_grads = torch.autograd.grad(gan_loss, last_layer_weight, retain_graph=True)[0]\n",
        "\n",
        "        Œª = torch.norm(perceptual_loss_grads) / (torch.norm(gan_loss_grads) + 1e-4)\n",
        "        Œª = torch.clamp(Œª, 0, 1e4).detach()\n",
        "        return 0.8 * Œª\n",
        "\n",
        "    @staticmethod\n",
        "    def adopt_weight(disc_factor, i, threshold, value=0.):\n",
        "        if i < threshold:\n",
        "            disc_factor = value\n",
        "        return disc_factor\n",
        "\n",
        "    def load_checkpoint(self, path, map_location=None):\n",
        "        \"\"\"Carga un checkpoint entrenado previamente de forma segura en CPU/GPU.\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=map_location or torch.device(\"cpu\"))\n",
        "        self.load_state_dict(checkpoint)\n",
        "        print(f\"‚úÖ Checkpoint cargado desde {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jABKsKW0rWKf"
      },
      "outputs": [],
      "source": [
        "# transformer.py\n",
        "class VQGANTransformer(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(VQGANTransformer, self).__init__()\n",
        "\n",
        "        self.sos_token = args.sos_token\n",
        "        self.vqgan = self.load_vqgan(args)\n",
        "\n",
        "        transformer_config = {\n",
        "            \"vocab_size\": args.num_codebook_vectors,\n",
        "            \"block_size\": 512,\n",
        "            \"n_layer\": 24,\n",
        "            \"n_head\": 16,\n",
        "            \"n_embd\": 1024\n",
        "        }\n",
        "        self.transformer = GPT(**transformer_config)\n",
        "\n",
        "        self.pkeep = args.pkeep\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vqgan(args):\n",
        "        model = VQGAN(args)\n",
        "        model.load_checkpoint(args.checkpoint_path)  # sin map_location\n",
        "        model = model.eval().to(\"cuda\")\n",
        "        return model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_to_z(self, x):\n",
        "        quant_z, indices, _ = self.vqgan.encode(x)\n",
        "        indices = indices.view(quant_z.shape[0], -1)\n",
        "        return quant_z, indices\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def z_to_image(self, indices, p1=16, p2=16):\n",
        "        dim = self.vqgan.codebook.embedding.embedding_dim\n",
        "        ix_to_vectors = self.vqgan.codebook.embedding(indices).reshape(indices.shape[0], p1, p2, dim)\n",
        "        ix_to_vectors = ix_to_vectors.permute(0, 3, 1, 2)\n",
        "        image = self.vqgan.decode(ix_to_vectors)\n",
        "        return image\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, indices = self.encode_to_z(x)\n",
        "\n",
        "        sos_tokens = torch.ones(x.shape[0], 1, device=\"cuda\") * self.sos_token\n",
        "        sos_tokens = sos_tokens.long()\n",
        "\n",
        "        mask = torch.bernoulli(self.pkeep * torch.ones(indices.shape, device=indices.device))\n",
        "        mask = mask.round().to(dtype=torch.int64)\n",
        "        random_indices = torch.randint_like(indices, self.transformer.config.vocab_size)\n",
        "        new_indices = mask * indices + (1 - mask) * random_indices\n",
        "\n",
        "        new_indices = torch.cat((sos_tokens, new_indices), dim=1)\n",
        "\n",
        "        target = indices\n",
        "\n",
        "        logits, _ = self.transformer(new_indices[:, :-1])\n",
        "\n",
        "        return logits, target\n",
        "\n",
        "    def top_k_logits(self, logits, k):\n",
        "        v, ix = torch.topk(logits, k)\n",
        "        out = logits.clone()\n",
        "        out[out < v[..., [-1]]] = -float(\"inf\")\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, x, c, steps, temperature=1.0, top_k=100):\n",
        "        self.transformer.eval()\n",
        "        x = torch.cat((c, x), dim=1)\n",
        "        for k in range(steps):\n",
        "            logits, _ = self.transformer(x)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                logits = self.top_k_logits(logits, top_k)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "            x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "        x = x[:, c.shape[1]:]\n",
        "        self.transformer.train()\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, x):\n",
        "        log = dict()\n",
        "\n",
        "        _, indices = self.encode_to_z(x)\n",
        "        sos_tokens = torch.ones(x.shape[0], 1, device=\"cuda\") * self.sos_token\n",
        "        sos_tokens = sos_tokens.long()\n",
        "\n",
        "        # Half-sample\n",
        "        start_indices = indices[:, :indices.shape[1] // 2]\n",
        "        sample_indices = self.sample(start_indices, sos_tokens, steps=indices.shape[1] - start_indices.shape[1])\n",
        "        half_sample = self.z_to_image(sample_indices)\n",
        "\n",
        "        # Full-sample\n",
        "        start_indices = indices[:, :0]\n",
        "        sample_indices = self.sample(start_indices, sos_tokens, steps=indices.shape[1])\n",
        "        full_sample = self.z_to_image(sample_indices)\n",
        "\n",
        "        # Reconstruction\n",
        "        x_rec = self.z_to_image(indices)\n",
        "\n",
        "        log[\"input\"] = x\n",
        "        log[\"rec\"] = x_rec\n",
        "        log[\"half_sample\"] = half_sample\n",
        "        log[\"full_sample\"] = full_sample\n",
        "\n",
        "        return log, torch.cat((x, x_rec, half_sample, full_sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxCiVX1mWF3J"
      },
      "source": [
        "#Metricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZR7VbffIxJat"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# FeatureStats class\n",
        "# =========================\n",
        "class FeatureStats:\n",
        "    def __init__(self, capture_mean_cov=True, max_items=None):\n",
        "        self.capture_mean_cov = capture_mean_cov\n",
        "        self.max_items = max_items\n",
        "        self.num_items = 0\n",
        "        self.num_features = None\n",
        "        self.raw_mean = None\n",
        "        self.raw_cov = None\n",
        "\n",
        "    def append_torch(self, x):\n",
        "        x = x.detach().cpu().numpy()\n",
        "        if self.num_features is None:\n",
        "            self.num_features = x.shape[1]\n",
        "            self.raw_mean = np.zeros([self.num_features], dtype=np.float64)\n",
        "            self.raw_cov = np.zeros([self.num_features, self.num_features], dtype=np.float64)\n",
        "        self.num_items += x.shape[0]\n",
        "\n",
        "        x64 = x.astype(np.float64)\n",
        "        self.raw_mean += x64.sum(axis=0)\n",
        "        self.raw_cov += x64.T @ x64\n",
        "\n",
        "    def get_mean_cov(self):\n",
        "        mean = self.raw_mean / self.num_items\n",
        "        cov = self.raw_cov / self.num_items - np.outer(mean, mean)\n",
        "        return mean, cov\n",
        "\n",
        "    def save(self, pkl_file):\n",
        "        with open(pkl_file, \"wb\") as f:\n",
        "            pickle.dump(self.__dict__, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(pkl_file):\n",
        "        with open(pkl_file, \"rb\") as f:\n",
        "            s = pickle.load(f)\n",
        "        obj = FeatureStats()\n",
        "        obj.__dict__.update(s)\n",
        "        return obj\n",
        "\n",
        "# =========================\n",
        "# Inception loader\n",
        "# =========================\n",
        "_inception_cache = {}\n",
        "\n",
        "def get_inception_v3(device=\"cuda\"):\n",
        "    if device not in _inception_cache:\n",
        "        inception = models.inception_v3(pretrained=True, transform_input=False)\n",
        "        inception.fc = torch.nn.Identity()  # quitar clasificador\n",
        "        inception.eval().to(device)\n",
        "        _inception_cache[device] = inception\n",
        "    return _inception_cache[device]\n",
        "\n",
        "# =========================\n",
        "# Compute dataset stats\n",
        "# =========================\n",
        "def compute_dataset_stats(dataset, device=\"cuda\", batch_size=64,\n",
        "                          cache_dir=\"/content/drive/MyDrive/Proyecto_Grado/VQ_GAN/VQCheckpointsV1\",\n",
        "                          max_items=None):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    # hash √∫nico para identificar dataset\n",
        "    dataset_id = str(dataset) + f\"-{len(dataset)}\"\n",
        "    md5 = hashlib.md5(dataset_id.encode(\"utf-8\")).hexdigest()\n",
        "    cache_file = os.path.join(cache_dir, f\"dataset_stats_{md5}.pkl\")\n",
        "\n",
        "    # Si ya existen stats guardados ‚Üí cargarlos\n",
        "    if os.path.isfile(cache_file):\n",
        "        print(f\"üìÇ Stats encontrados: {cache_file}\")\n",
        "        return FeatureStats.load(cache_file)\n",
        "\n",
        "    print(\"üîé Calculando stats del dataset real...\")\n",
        "    inception = get_inception_v3(device)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    stats = FeatureStats(capture_mean_cov=True, max_items=max_items)\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Extrayendo features\"):\n",
        "        if isinstance(batch, (list, tuple)):\n",
        "            images = batch[0]\n",
        "        else:\n",
        "            images = batch\n",
        "\n",
        "        # Si son grayscale ‚Üí duplicar canales\n",
        "        if images.shape[1] == 1:\n",
        "            images = images.repeat(1, 3, 1, 1)\n",
        "\n",
        "        # Ajustar tama√±o para InceptionV3\n",
        "        images = F.interpolate(images, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feats = inception(images.to(device))\n",
        "        stats.append_torch(feats)\n",
        "\n",
        "    stats.save(cache_file)\n",
        "    print(f\"‚úÖ Stats guardados en: {cache_file}\")\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CzwoCU4yt4Io",
        "outputId": "dfa58233-e7fc-411b-d458-759c343b18dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Stats ya existen, cargando desde: /content/drive/MyDrive/Proyecto_Grado/VQ_GAN/VQCheckpointsVQ1/dataset_stats_c40b27d8db3f4b1d1ede4a5413f38844.pkl\n",
            "mu_real: (2048,) sigma_real: (2048, 2048)\n"
          ]
        }
      ],
      "source": [
        "stats_dir = \"/content/drive/MyDrive/Proyecto_Grado/VQ_GAN/VQCheckpointsVQ1\"\n",
        "stats_path = os.path.join(stats_dir, \"dataset_stats_c40b27d8db3f4b1d1ede4a5413f38844.pkl\")\n",
        "\n",
        "# Transformaciones del dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(\n",
        "    \"/content/drive/MyDrive/Proyecto_Grado/Data/frames_extraidos_MedGAN\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# === Revisi√≥n del archivo de stats ===\n",
        "if os.path.exists(stats_path):\n",
        "    print(f\"‚úÖ Stats ya existen, cargando desde: {stats_path}\")\n",
        "    with open(stats_path, \"rb\") as f:\n",
        "        stats = pickle.load(f)  # Esto es un dict\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Stats no encontrados, calculando desde cero...\")\n",
        "    stats = compute_dataset_stats(dataset, device=\"cuda\", batch_size=64)\n",
        "    with open(stats_path, \"wb\") as f:\n",
        "        pickle.dump(stats, f)\n",
        "\n",
        "# Usar el diccionario directamente\n",
        "mu_real = np.array(stats[\"raw_mean\"])\n",
        "sigma_real = np.array(stats[\"raw_cov\"])\n",
        "\n",
        "print(\"mu_real:\", mu_real.shape, \"sigma_real:\", sigma_real.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YVGNVMf3qtXd"
      },
      "outputs": [],
      "source": [
        "# LPIPS.py\n",
        "URL_MAP = {\n",
        "    \"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"\n",
        "}\n",
        "\n",
        "CKPT_MAP = {\n",
        "    \"vgg_lpips\": \"vgg.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "def download(url, local_path, chunk_size=1024):\n",
        "    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        total_size = int(r.headers.get(\"content-length\", 0))\n",
        "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n",
        "            with open(local_path, \"wb\") as f:\n",
        "                for data in r.iter_content(chunk_size=chunk_size):\n",
        "                    if data:\n",
        "                        f.write(data)\n",
        "                        pbar.update(chunk_size)\n",
        "\n",
        "\n",
        "def get_ckpt_path(name, root):\n",
        "    assert name in URL_MAP\n",
        "    path = os.path.join(root, CKPT_MAP[name])\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Downloading {name} model from {URL_MAP[name]} to {path}\")\n",
        "        download(URL_MAP[name], path)\n",
        "    return path\n",
        "\n",
        "\n",
        "class LPIPS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LPIPS, self).__init__()\n",
        "        self.scaling_layer = ScalingLayer()\n",
        "        self.channels = [64, 128, 256, 512, 512]\n",
        "        self.vgg = VGG16()\n",
        "        self.lins = nn.ModuleList([\n",
        "            NetLinLayer(self.channels[0]),\n",
        "            NetLinLayer(self.channels[1]),\n",
        "            NetLinLayer(self.channels[2]),\n",
        "            NetLinLayer(self.channels[3]),\n",
        "            NetLinLayer(self.channels[4])\n",
        "        ])\n",
        "\n",
        "        self.load_from_pretrained()\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def load_from_pretrained(self, name=\"vgg_lpips\"):\n",
        "        ckpt = get_ckpt_path(name, \"vgg_lpips\")\n",
        "        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
        "\n",
        "    def forward(self, real_x, fake_x):\n",
        "        # üî• Normalizar tama√±o si no coincide\n",
        "        if real_x.shape[2:] != fake_x.shape[2:]:\n",
        "            fake_x = F.interpolate(fake_x, size=real_x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        features_real = self.vgg(self.scaling_layer(real_x))\n",
        "        features_fake = self.vgg(self.scaling_layer(fake_x))\n",
        "        diffs = {}\n",
        "\n",
        "        for i in range(len(self.channels)):\n",
        "            diffs[i] = (norm_tensor(features_real[i]) - norm_tensor(features_fake[i])) ** 2\n",
        "\n",
        "        return sum([spatial_average(self.lins[i].model(diffs[i])) for i in range(len(self.channels))])\n",
        "\n",
        "\n",
        "class ScalingLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScalingLayer, self).__init__()\n",
        "        self.register_buffer(\"shift\", torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n",
        "        self.register_buffer(\"scale\", torch.Tensor([.458, .448, .450])[None, :, None, None])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x - self.shift) / self.scale\n",
        "\n",
        "\n",
        "class NetLinLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=1):\n",
        "        super(NetLinLayer, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
        "        )\n",
        "\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        vgg_pretrained_features = vgg16(pretrained=True).features\n",
        "        slices = [vgg_pretrained_features[i] for i in range(30)]\n",
        "        self.slice1 = nn.Sequential(*slices[0:4])\n",
        "        self.slice2 = nn.Sequential(*slices[4:9])\n",
        "        self.slice3 = nn.Sequential(*slices[9:16])\n",
        "        self.slice4 = nn.Sequential(*slices[16:23])\n",
        "        self.slice5 = nn.Sequential(*slices[23:30])\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.slice1(x)\n",
        "        h_relu1 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4 = h\n",
        "        h = self.slice5(h)\n",
        "        h_relu5 = h\n",
        "        vgg_outputs = namedtuple(\"VGGOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
        "        return vgg_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n",
        "\n",
        "\n",
        "def norm_tensor(x):\n",
        "    \"\"\"\n",
        "    Normalize images by their length to make them unit vector?\n",
        "    :param x: batch of images\n",
        "    :return: normalized batch of images\n",
        "    \"\"\"\n",
        "    norm_factor = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n",
        "    return x / (norm_factor + 1e-10)\n",
        "\n",
        "\n",
        "def spatial_average(x):\n",
        "    \"\"\"\n",
        "     imgs have: batch_size x channels x width x height --> average over width and height channel\n",
        "    :param x: batch of images\n",
        "    :return: averaged images along width and height\n",
        "    \"\"\"\n",
        "    return x.mean([2, 3], keepdim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xWwZxnHslG7"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHKiWWEtRN5"
      },
      "source": [
        "## Entrenamiento VQ-GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xqJO1L8vrhRr"
      },
      "outputs": [],
      "source": [
        "# training_vqgan.py\n",
        "class TrainVQGAN:\n",
        "    def __init__(self, args, base_ckpt_dir, loss_vq_path, metrics_path):\n",
        "        self.vqgan = VQGAN(args).to(device=args.device)\n",
        "        self.discriminator = Discriminator(args).to(device=args.device)\n",
        "        self.discriminator.apply(weights_init)\n",
        "\n",
        "        # LPIPS\n",
        "        self.perceptual_loss = lpips.LPIPS(net='alex').eval().to(device=args.device)\n",
        "\n",
        "        # Rutas de logs\n",
        "        self.loss_vq_path = loss_vq_path\n",
        "        self.metrics_path = metrics_path\n",
        "\n",
        "        # Optimizers\n",
        "        self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n",
        "\n",
        "        # ================================\n",
        "        # 1. Detectar √∫ltima carpeta de checkpoints\n",
        "        # ================================\n",
        "        existing = [d for d in os.listdir(base_ckpt_dir) if re.match(r\"^\\d{5}_checkpoint$\", d)]\n",
        "        if existing:\n",
        "            last_num = max([int(d.split(\"_\")[0]) for d in existing])\n",
        "            last_folder = f\"{last_num:05d}_checkpoint\"\n",
        "            last_path = os.path.join(base_ckpt_dir, last_folder)\n",
        "\n",
        "            # Buscar √∫ltimo .pt dentro de esa carpeta\n",
        "            ckpts = [f for f in os.listdir(last_path) if f.startswith(\"vqgan_epoch_\") and f.endswith(\".pt\")]\n",
        "            if ckpts:\n",
        "                ckpts.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "                last_ckpt = ckpts[-1]\n",
        "                ckpt_path = os.path.join(last_path, last_ckpt)\n",
        "                self.vqgan.load_state_dict(torch.load(ckpt_path, map_location=args.device))\n",
        "                self.start_epoch = int(last_ckpt.split(\"_\")[-1].split(\".\")[0])\n",
        "                print(f\"‚úÖ √öltimo checkpoint cargado: {ckpt_path} (√©poca {self.start_epoch})\")\n",
        "            else:\n",
        "                self.start_epoch = 0\n",
        "                print(\"‚ö†Ô∏è No se encontraron .pt en la √∫ltima carpeta, entrenamiento desde cero.\")\n",
        "        else:\n",
        "            last_num, self.start_epoch = -1, 0\n",
        "            print(\"‚ö†Ô∏è No se encontraron carpetas de checkpoints previas, entrenamiento desde cero.\")\n",
        "\n",
        "        # ================================\n",
        "        # 2. Crear nueva carpeta de run\n",
        "        # ================================\n",
        "        new_num = last_num + 1\n",
        "        run_folder = f\"{new_num:05d}_checkpoint\"\n",
        "        self.run_path = os.path.join(base_ckpt_dir, run_folder)\n",
        "        os.makedirs(self.run_path, exist_ok=True)\n",
        "        print(f\"üìÇ Carpeta de checkpoints actual: {self.run_path}\")\n",
        "\n",
        "        # ================================\n",
        "        # 3. Cargar historial global de p√©rdidas y m√©tricas\n",
        "        # ================================\n",
        "        self.loss_history = []\n",
        "        if os.path.exists(self.loss_vq_path):\n",
        "            with open(self.loss_vq_path, \"r\") as f:\n",
        "                self.loss_history = json.load(f)\n",
        "\n",
        "        self.metric_history = []\n",
        "        if os.path.exists(self.metrics_path):\n",
        "            with open(self.metrics_path, \"r\") as f:\n",
        "                self.metric_history = json.load(f)\n",
        "\n",
        "    def configure_optimizers(self, args):\n",
        "        lr = args.learning_rate\n",
        "        opt_vq = torch.optim.Adam(\n",
        "            list(self.vqgan.encoder.parameters()) +\n",
        "            list(self.vqgan.decoder.parameters()) +\n",
        "            list(self.vqgan.codebook.parameters()) +\n",
        "            list(self.vqgan.quant_conv.parameters()) +\n",
        "            list(self.vqgan.post_quant_conv.parameters()),\n",
        "            lr=lr, eps=1e-08, betas=(args.beta1, args.beta2)\n",
        "        )\n",
        "        opt_disc = torch.optim.Adam(\n",
        "            self.discriminator.parameters(),\n",
        "            lr=lr, eps=1e-08, betas=(args.beta1, args.beta2)\n",
        "        )\n",
        "        return opt_vq, opt_disc\n",
        "\n",
        "    def compute_metrics(self, real, recon):\n",
        "        # Pasar a [0,1] y CPU\n",
        "        real_np = ((real.detach().cpu().numpy() + 1) / 2).clip(0, 1)\n",
        "        recon_np = ((recon.detach().cpu().numpy() + 1) / 2).clip(0, 1)\n",
        "\n",
        "        # convertir a formato NCHW -> NHWC para skimage\n",
        "        real_np = np.transpose(real_np, (0, 2, 3, 1))\n",
        "        recon_np = np.transpose(recon_np, (0, 2, 3, 1))\n",
        "\n",
        "        ssim_vals, psnr_vals = [], []\n",
        "        for r, f in zip(real_np, recon_np):\n",
        "            ssim_vals.append(ssim_metric(r, f, channel_axis=-1, data_range=1.0))\n",
        "            psnr_vals.append(psnr_metric(r, f, data_range=1.0))\n",
        "\n",
        "        # LPIPS batch directo\n",
        "        lpips_val = self.perceptual_loss(real, recon).mean().item()\n",
        "\n",
        "        return {\n",
        "            \"ssim\": float(np.mean(ssim_vals)),\n",
        "            \"psnr\": float(np.mean(psnr_vals)),\n",
        "            \"lpips\": float(lpips_val)\n",
        "        }\n",
        "\n",
        "    def train(self, args):\n",
        "        train_dataset = load_data(args)\n",
        "        steps_per_epoch = len(train_dataset)\n",
        "        global_step, Œª_prev = 0, 1.0\n",
        "\n",
        "        for epoch in range(self.start_epoch, args.epochs):\n",
        "            rec_losses, perceptual_losses, q_losses = [], [], []\n",
        "            g_losses, gen_losses, disc_losses = [], [], []\n",
        "\n",
        "            with tqdm(range(len(train_dataset))) as pbar:\n",
        "                for i, imgs in zip(pbar, train_dataset):\n",
        "                    imgs = imgs.to(device=args.device)\n",
        "                    decoded_images, _, q_loss = self.vqgan(imgs)\n",
        "\n",
        "                    if imgs.shape != decoded_images.shape:\n",
        "                        decoded_images = F.interpolate(decoded_images, size=imgs.shape[2:], mode=\"bilinear\")\n",
        "\n",
        "                    # discriminador\n",
        "                    disc_real = self.discriminator(imgs)\n",
        "                    disc_fake = self.discriminator(decoded_images)\n",
        "\n",
        "                    disc_factor = self.vqgan.adopt_weight(\n",
        "                        args.disc_factor, epoch * steps_per_epoch + i, threshold=args.disc_start\n",
        "                    )\n",
        "\n",
        "                    # p√©rdidas\n",
        "                    if global_step % args.lpips_interval == 0:\n",
        "                        perceptual_loss = self.perceptual_loss(imgs, decoded_images).mean()\n",
        "                    else:\n",
        "                        perceptual_loss = torch.tensor(0.0, device=args.device)\n",
        "\n",
        "                    rec_loss = torch.abs(imgs - decoded_images).mean()\n",
        "                    perceptual_rec_loss = (\n",
        "                        args.perceptual_loss_factor * perceptual_loss +\n",
        "                        args.rec_loss_factor * rec_loss\n",
        "                    )\n",
        "\n",
        "                    g_loss = -torch.mean(disc_fake)\n",
        "                    Œª = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss) if (global_step % 50) == 0 else Œª_prev\n",
        "                    Œª_prev = Œª\n",
        "\n",
        "                    vq_loss = perceptual_rec_loss + q_loss + disc_factor * Œª * g_loss\n",
        "                    d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
        "                    d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
        "                    gan_loss = disc_factor * 0.5 * (d_loss_real + d_loss_fake)\n",
        "\n",
        "                    # optimizaci√≥n\n",
        "                    self.opt_vq.zero_grad()\n",
        "                    vq_loss.backward(retain_graph=True)\n",
        "                    self.opt_disc.zero_grad()\n",
        "                    gan_loss.backward()\n",
        "                    self.opt_vq.step()\n",
        "                    self.opt_disc.step()\n",
        "\n",
        "                    # acumular\n",
        "                    rec_losses.append(rec_loss.item())\n",
        "                    perceptual_losses.append(perceptual_loss.item())\n",
        "                    q_losses.append(q_loss.item())\n",
        "                    g_losses.append(g_loss.item())\n",
        "                    gen_losses.append(vq_loss.item())\n",
        "                    disc_losses.append(gan_loss.item())\n",
        "\n",
        "                    pbar.set_postfix(\n",
        "                        Rec=np.round(rec_loss.item(), 5),\n",
        "                        Perc=np.round(perceptual_loss.item(), 5),\n",
        "                        Q=np.round(q_loss.item(), 5),\n",
        "                        G=np.round(g_loss.item(), 5),\n",
        "                        Gen=np.round(vq_loss.item(), 5),\n",
        "                        Disc=np.round(gan_loss.item(), 5),\n",
        "                    )\n",
        "                    global_step += 1\n",
        "\n",
        "            # Guardar historial global\n",
        "            self.loss_history.append({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"rec_loss\": float(np.mean(rec_losses)),\n",
        "                \"perceptual_loss\": float(np.mean(perceptual_losses)),\n",
        "                \"q_loss\": float(np.mean(q_losses)),\n",
        "                \"g_loss\": float(np.mean(g_losses)),\n",
        "                \"gen_loss\": float(np.mean(gen_losses)),\n",
        "                \"disc_loss\": float(np.mean(disc_losses)),\n",
        "            })\n",
        "            with open(self.loss_vq_path, \"w\") as f:\n",
        "                json.dump(self.loss_history, f, indent=4)\n",
        "\n",
        "            # === Guardar checkpoint, imagen y m√©tricas cada 5 √©pocas ===\n",
        "            if (epoch + 1) % 1 == 0:\n",
        "                ckpt_path = os.path.join(self.run_path, f\"vqgan_epoch_{epoch+1}.pt\")\n",
        "                torch.save(self.vqgan.state_dict(), ckpt_path)\n",
        "                print(f\"üíæ Checkpoint guardado: {ckpt_path}\")\n",
        "\n",
        "                gen_path = os.path.join(GEN_IMG_DIR, f\"generated_v1_{epoch+1}.png\")\n",
        "                vutils.save_image((decoded_images[0].detach().cpu() + 1) * 0.5, gen_path)\n",
        "                print(f\"üñº Imagen generada guardada: {gen_path}\")\n",
        "\n",
        "                metrics = self.compute_metrics(imgs[:8], decoded_images[:8])\n",
        "                metrics[\"epoch\"] = epoch + 1\n",
        "                self.metric_history.append(metrics)\n",
        "                with open(self.metrics_path, \"w\") as f:\n",
        "                    json.dump(self.metric_history, f, indent=4)\n",
        "                print(f\"üìä M√©tricas guardadas: {metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Xve94Sa7tU_S"
      },
      "outputs": [],
      "source": [
        "# === Configuraci√≥n de args ===\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--latent-dim', type=int, default=128)\n",
        "parser.add_argument('--image-size', type=int, default=128)\n",
        "parser.add_argument('--num-codebook-vectors', type=int, default=512)\n",
        "parser.add_argument('--beta', type=float, default=0.25)\n",
        "parser.add_argument('--image-channels', type=int, default=3)\n",
        "parser.add_argument('--dataset-path', type=str, default='/content/dataset')\n",
        "parser.add_argument('--device', type=str, default=\"cuda\")\n",
        "parser.add_argument('--batch-size', type=int, default=8)\n",
        "parser.add_argument('--epochs', type=int, default=1000)\n",
        "parser.add_argument('--learning-rate', type=float, default=2.25e-05)\n",
        "parser.add_argument('--beta1', type=float, default=0.5)\n",
        "parser.add_argument('--beta2', type=float, default=0.9)\n",
        "parser.add_argument('--disc-start', type=int, default=10000)\n",
        "parser.add_argument('--disc-factor', type=float, default=1.)\n",
        "parser.add_argument('--rec-loss-factor', type=float, default=1.)\n",
        "parser.add_argument('--perceptual-loss-factor', type=float, default=1.)\n",
        "parser.add_argument(\"--lpips_interval\", type=int, default=5,\n",
        "                    help=\"Cada cu√°ntos pasos calcular LPIPS perceptual loss.\")\n",
        "args = parser.parse_args([])\n",
        "\n",
        "# Forzar GPU (siempre)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Dataset path\n",
        "data_dir = \"/content/drive/MyDrive/Proyecto_Grado/Data\"\n",
        "args.dataset_path = f\"{data_dir}/frames_extraidos\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZvjH4b9dCj1",
        "outputId": "573c8387-2dd1-4cad-cea2-db580b07971f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "‚ö†Ô∏è No se encontraron carpetas de checkpoints previas, entrenamiento desde cero.\n",
            "üìÇ Carpeta de checkpoints actual: /content/drive/MyDrive/Proyecto_Grado/VQ_GAN/VQCheckpointsVQ1/00000_checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "  1%|          | 50/5015 [00:56<1:26:54,  1.05s/it, Disc=0, G=-0.0564, Gen=0.93, Perc=0, Q=0.827, Rec=0.103]"
          ]
        }
      ],
      "source": [
        "# Solo defines el directorio base donde estar√°n todas las carpetas\n",
        "BASE_CKPT_DIR = \"/content/drive/MyDrive/Proyecto_Grado/VQ_GAN/VQCheckpointsVQ1\"\n",
        "train_vqgan = TrainVQGAN(args, BASE_CKPT_DIR, LOSS_VQ_PATH, METRICS_PATH)\n",
        "\n",
        "# Entrenar directamente, sin preocuparte por start_epoch\n",
        "train_vqgan.train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWRnDyEQtaRI"
      },
      "source": [
        "## Entrenamiento del Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svqdDtJerwEG"
      },
      "outputs": [],
      "source": [
        "# training_transformer.py\n",
        "class TrainTransformer:\n",
        "    def __init__(self, args, run_path):\n",
        "        self.model = VQGANTransformer(args).to(device=args.device)\n",
        "        self.optim = self.configure_optimizers()\n",
        "        self.run_path = run_path\n",
        "        os.makedirs(self.run_path, exist_ok=True)\n",
        "        self.train(args)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        decay, no_decay = set(), set()\n",
        "        whitelist_weight_modules = (nn.Linear,)\n",
        "        blacklist_weight_modules = (nn.LayerNorm, nn.Embedding)\n",
        "\n",
        "        for mn, m in self.model.transformer.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = f\"{mn}.{pn}\" if mn else pn\n",
        "                if pn.endswith(\"bias\"):\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith(\"weight\") and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith(\"weight\") and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        no_decay.add(\"pos_emb\")\n",
        "        param_dict = {pn: p for pn, p in self.model.transformer.named_parameters()}\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": 0.01},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        return torch.optim.AdamW(optim_groups, lr=4.5e-06, betas=(0.9, 0.95))\n",
        "\n",
        "    def train(self, args, start_epoch=0):\n",
        "      train_dataset = load_data(args)\n",
        "      steps_per_epoch = len(train_dataset)\n",
        "      global_step, Œª_prev = 0, 1.0\n",
        "\n",
        "      for epoch in range(args.epochs):\n",
        "          epoch_losses = []\n",
        "          with tqdm(range(len(train_dataset))) as pbar:\n",
        "              for i, imgs in zip(pbar, train_dataset):\n",
        "                  self.optim.zero_grad()\n",
        "                  imgs = imgs.to(device=args.device)\n",
        "                  logits, targets = self.model(imgs)\n",
        "                  loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "                  loss.backward()\n",
        "                  self.optim.step()\n",
        "                  epoch_losses.append(loss.item())\n",
        "                  pbar.set_postfix(Transformer_Loss=np.round(loss.item(), 4))\n",
        "                  pbar.update(0)\n",
        "\n",
        "          # === Guardar historia de p√©rdidas ===\n",
        "          loss_history.append({\"epoch\": epoch + 1, \"loss\": float(np.mean(epoch_losses))})\n",
        "          with open(os.path.join(self.run_path, \"transformer_loss_history.json\"), \"w\") as f:\n",
        "              json.dump(loss_history, f, indent=4)\n",
        "\n",
        "          # === Cada 5 √©pocas: checkpoint, imagen y m√©tricas ===\n",
        "          if (epoch + 1) % 5 == 0:\n",
        "              ckpt_path = os.path.join(self.run_path, f\"transformer_epoch_{epoch+1}.pt\")\n",
        "              torch.save(self.model.state_dict(), ckpt_path)\n",
        "              print(f\"üíæ Checkpoint guardado: {ckpt_path}\")\n",
        "\n",
        "              # Generar imagen de muestra\n",
        "              log, sampled_imgs = self.model.log_images(imgs[0][None])\n",
        "              sample_path = os.path.join(self.run_path, f\"transformer_sample_epoch_{epoch+1}.jpg\")\n",
        "              vutils.save_image(sampled_imgs, sample_path, nrow=4)\n",
        "              print(f\"üñº Imagen de muestra guardada: {sample_path}\")\n",
        "\n",
        "              # Calcular m√©tricas\n",
        "              decoded_images = self.model.z_to_image(self.model.vqgan.encode_codebook(imgs))\n",
        "              metrics_path = os.path.join(self.run_path, f\"transformer_metrics_epoch_{epoch+1}.json\")\n",
        "              compute_metrics(imgs[:8].detach(), decoded_images[:8].detach(), metrics_path, stage=\"transformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tnvpgS2tekP"
      },
      "outputs": [],
      "source": [
        "# === Crear carpeta numerada para el entrenamiento del Transformer ===\n",
        "base_ckpt_dir = \"/content/drive/MyDrive/Proyecto_Grado/VQ_GAN/TransformerCheckpointsV1\"\n",
        "os.makedirs(base_ckpt_dir, exist_ok=True)\n",
        "\n",
        "existing = [d for d in os.listdir(base_ckpt_dir) if re.match(r\"^\\d{5}_checkpoint$\", d)]\n",
        "if existing:\n",
        "    last_num = max([int(d.split(\"_\")[0]) for d in existing])\n",
        "    new_num = last_num + 1\n",
        "else:\n",
        "    new_num = 0\n",
        "\n",
        "run_folder = f\"{new_num:05d}_checkpoint\"\n",
        "run_path = os.path.join(base_ckpt_dir, run_folder)\n",
        "os.makedirs(run_path, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Carpeta de entrenamiento de Transformer creada: {run_path}\")\n",
        "\n",
        "# === Configuraci√≥n de args ===\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--latent-dim', type=int, default=128)\n",
        "parser.add_argument('--image-size', type=int, default=128)\n",
        "parser.add_argument('--num-codebook-vectors', type=int, default=512)\n",
        "parser.add_argument('--beta', type=float, default=0.25)\n",
        "parser.add_argument('--image-channels', type=int, default=3)\n",
        "parser.add_argument('--dataset-path', type=str, default='/content/dataset')\n",
        "parser.add_argument('--checkpoint-path', type=str, default='./checkpoints/vqgan_last_ckpt.pt')\n",
        "parser.add_argument('--device', type=str, default=\"cuda\")\n",
        "parser.add_argument('--batch-size', type=int, default=20)\n",
        "parser.add_argument('--epochs', type=int, default=1000)\n",
        "parser.add_argument('--learning-rate', type=float, default=2.25e-05)\n",
        "parser.add_argument('--beta1', type=float, default=0.5)\n",
        "parser.add_argument('--beta2', type=float, default=0.9)\n",
        "parser.add_argument('--disc-start', type=int, default=10000)\n",
        "parser.add_argument('--disc-factor', type=float, default=1.0)\n",
        "parser.add_argument('--l2-loss-factor', type=float, default=1.)\n",
        "parser.add_argument('--perceptual-loss-factor', type=float, default=1.)\n",
        "parser.add_argument('--pkeep', type=float, default=0.5)\n",
        "parser.add_argument('--sos-token', type=int, default=0)\n",
        "args = parser.parse_args([])\n",
        "\n",
        "# Dataset\n",
        "args.dataset_path = os.path.join(BASE_DIR, \"frames_extraidos\")\n",
        "\n",
        "# Entrenar Transformer\n",
        "train_transformer = TrainTransformer(args)\n",
        "train_transformer.run_path = run_path  # <= IMPORTANTE: Para que sepa d√≥nde guardar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQE7kDEh0FF9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvn9x2Jm9a4v"
      },
      "source": [
        "#Generar Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "MKiCk6UKs-RK",
        "outputId": "7cc96017-7c49-4d94-e78d-0c96eeae14b9"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Proyecto_Grado/Data/Checkpoints/vqgan_epoch_100.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4127451737.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Cargar el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVQGANTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mtransformer_ckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_checkpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformer_epoch_100.pt\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ajusta si usas otro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_ckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3843065796.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vqgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         transformer_config = {\n",
            "\u001b[0;32m/tmp/ipython-input-3843065796.py\u001b[0m in \u001b[0;36mload_vqgan\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_vqgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVQGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3081779563.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Proyecto_Grado/Data/Checkpoints/vqgan_epoch_100.pt'"
          ]
        }
      ],
      "source": [
        "# sample_tranformer.py\n",
        "import os\n",
        "import torch\n",
        "from torchvision import utils as vutils\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Usa las mismas rutas que definiste en las celdas de entrenamiento\n",
        "data_dir = \"/content/drive/MyDrive/Proyecto_Grado/Data\"\n",
        "output_checkpoints = f\"{data_dir}/Checkpoints\"\n",
        "results_dir = f\"{data_dir}/Resultados_Transformer\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Configuraci√≥n manual de los par√°metros (sin argparse)\n",
        "class Args:\n",
        "    latent_dim = 256\n",
        "    image_size = 256\n",
        "    num_codebook_vectors = 1024\n",
        "    beta = 0.25\n",
        "    image_channels = 3\n",
        "    dataset_path = f\"{data_dir}/frames_extraidos_MedGAN\"\n",
        "    checkpoint_path = os.path.join(output_checkpoints, \"vqgan_epoch_100.pt\")  # Ajusta si tienes otro checkpoint\n",
        "    device = \"cuda\"\n",
        "    batch_size = 20\n",
        "    epochs = 100\n",
        "    learning_rate = 2.25e-05\n",
        "    beta1 = 0.5\n",
        "    beta2 = 0.9\n",
        "    disc_start = 10000\n",
        "    disc_factor = 1.\n",
        "    l2_loss_factor = 1.\n",
        "    perceptual_loss_factor = 1.\n",
        "    pkeep = 0.5\n",
        "    sos_token = 0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Cargar el modelo\n",
        "transformer = VQGANTransformer(args).to(args.device)\n",
        "transformer_ckpt_path = os.path.join(output_checkpoints, \"transformer_epoch_100.pt\")  # Ajusta si usas otro\n",
        "transformer.load_state_dict(torch.load(transformer_ckpt_path, map_location=args.device))\n",
        "print(f\"‚úÖ Loaded Transformer checkpoint: {transformer_ckpt_path}\")\n",
        "\n",
        "# Generar N im√°genes\n",
        "n = 10  # Cambia el n√∫mero de im√°genes a generar\n",
        "for i in tqdm(range(n)):\n",
        "    start_indices = torch.zeros((4, 0)).long().to(args.device)\n",
        "    sos_tokens = torch.ones(start_indices.shape[0], 1) * args.sos_token\n",
        "    sos_tokens = sos_tokens.long().to(args.device)\n",
        "    sample_indices = transformer.sample(start_indices, sos_tokens, steps=256)\n",
        "    sampled_imgs = transformer.z_to_image(sample_indices)\n",
        "\n",
        "    save_path = os.path.join(results_dir, f\"transformer_sample_{i}.jpg\")\n",
        "    vutils.save_image(sampled_imgs, save_path, nrow=4)\n",
        "    print(f\"üíæ Saved: {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "a_PYzx5zAmkY",
        "VxBA0wscpM8_",
        "cxCiVX1mWF3J",
        "HWRnDyEQtaRI",
        "rvn9x2Jm9a4v"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}